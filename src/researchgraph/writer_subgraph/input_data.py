writer_subgraph_input_data = {
    "new_method": """
Adaptive Curvature Momentum (ACM) Optimizer Overview Existing adaptive optimizers such as Adam and AdaBelief dynamically adjust the learning rate based on the history of gradients. However, while these methods adapt to the magnitude of the gradients, they do not fully exploit information about the local curvature of the loss landscape. In this proposal, we introduce a new optimizer called Adaptive Curvature Momentum (ACM), which utilizes local quadratic approximations to adaptively adjust the update direction and scale. Method Standard Momentum Update Similar to SGD or Adam, ACM maintains a momentum term based on past gradients. Adaptive Learning Rate Scaling Uses second-order information (approximations of the Hessian) to dynamically adjust the learning rate for each direction. To reduce the computational cost of Hessian calculations, Fisher Information Matrix approximations can be employed. Curvature-Aware Adaptive Adjustment Estimates curvature by using the gradient change rate: Œî ùëî = ùëî ùë° ‚àí ùëî ùë° ‚àí 1 Œîg=g t ‚Äã ‚àíg t‚àí1 ‚Äã Modifies the learning rate based on curvature: ùúÇ ùë° = ùõº 1 + ùõΩ ‚ãÖ Curvature ( ùëî ùë° ) Œ∑ t ‚Äã = 1+Œ≤‚ãÖCurvature(g t ‚Äã ) Œ± ‚Äã where ùõº Œ± is the base learning rate, and ùõΩ Œ≤ controls the influence of curvature. Adaptive Regularization Encourages stable updates by incorporating an adaptive weight decay mechanism. When local curvature is high, the optimizer strengthens regularization to suppress excessive updates. Key Features and Benefits Combines Adam-style adaptability with curvature-aware updates. Faster convergence: Adapts step sizes dynamically, taking larger steps in flat regions and smaller steps in sharp valleys. Hessian-free approximation: Utilizes efficient curvature estimation while maintaining low computational overhead. Scalability: Suitable for large-scale models such as ResNets and Transformers.
""",
    "verification_policy": "",
    "experiment_details": "",
    "experiment_code": """
#!/usr/bin/env python This script contains the following experiments: 1. Synthetic Optimization Benchmark (Convex Quadratic and Rosenbrock-like functions) 2. Deep Neural Network Training on CIFAR-10 using a simple CNN 3. Ablation Study & Hyperparameter Sensitivity Analysis on MNIST Each experiment compares a custom Adaptive Curvature Momentum (ACM) optimizer against established optimizers (Adam, SGD with momentum). The ACM optimizer adjusts per-parameter learning rates using a simple curvature-estimate (the difference between successive gradients) and uses momentum buffering. A quick_test() function is provided to run minimal iterations (to verify code execution). import torch import torch.nn as nn import torch.optim as optim import torchvision import torchvision.transforms as transforms import matplotlib.pyplot as plt import numpy as np import time ################################################################################ # 1. Synthetic Optimization Benchmark ################################################################################ # Define the ACM optimizer (for synthetic experiments and later deep-learning tests) class ACMOptimizer(optim.Optimizer): def __init__(self, params, lr=0.01, beta=0.9, curvature_influence=0.1): # The momentum_buffer variable will be stored in state, so we do not initialize it in defaults. defaults = dict(lr=lr, beta=beta, curvature_influence=curvature_influence) super(ACMOptimizer, self).__init__(params, defaults) def step(self, closure=None): loss = None if closure is not None: loss = closure() # Only single parameter group is assumed in this simple implementation. for group in self.param_groups: lr = group['lr'] beta = group['beta'] curvature_influence = group['curvature_influence'] for p in group['params']: if p.grad is None: continue grad = p.grad.data state = self.state[p] if 'momentum_buffer' not in state: # For the very first update, simply store the current gradient. state['momentum_buffer'] = torch.clone(grad).detach() p.data.add_(-lr * grad) else: buf = state['momentum_buffer'] # Estimate curvature simply as the absolute difference between current gradient and previous momentum. curvature_est = (grad - buf).abs() # Compute an adaptive per-component learning rate. adaptive_lr = lr / (1.0 + curvature_influence * curvature_est) # Update the momentum buffer with exponential moving average. buf.mul_(beta).add_(grad, alpha=1 - beta) # Update parameters using (elementwise) adaptive learning rate and momentum buffer. p.data.add_(-adaptive_lr * buf) return loss # Define a convex quadratic function: f(x) = 0.5 * x^T A x - b^T x. def quadratic_loss(x, A, b): return 0.5 * x @ A @ x - b @ x # Define a modified Rosenbrock-like function (a simple nonconvex function) def rosenbrock_loss(x): # Here x is assumed to be a 2D tensor. a = 1.0 b = 100.0 return (a - x[0])**2 + b * (x[1] - x[0]**2)**2 def run_synthetic_experiment(num_iters=100): print("=== Synthetic Experiment: Quadratic Function Optimization ===") torch.manual_seed(0) # Example 2D quadratic. A is positive definite. A = torch.tensor([[3.0, 0.2], [0.2, 2.0]]) b = torch.tensor([1.0, 1.0]) # Prepare optimizers for each method. optimizers_dict = { "ACM": ACMOptimizer, "Adam": optim.Adam, "SGD_mom": lambda params, lr: optim.SGD(params, lr=lr, momentum=0.9) } results = {name: [] for name in optimizers_dict.keys()} # Run separate optimization runs for each optimizer starting with a new initial point. for name, opt_class in optimizers_dict.items(): print(f"\nRunning optimization with {name}") # reinitialize the initial point for fairness x_data = torch.randn(2, requires_grad=True) # Instantiate the optimizer. if name == "ACM": optimizer = opt_class([x_data], lr=0.1, beta=0.9, curvature_influence=0.05) else: optimizer = opt_class([x_data], lr=0.1) for i in range(num_iters): optimizer.zero_grad() loss = quadratic_loss(x_data, A, b) loss.backward() optimizer.step() results[name].append(loss.item()) if (i + 1) % (num_iters // 5) == 0 or i == 0: print(f"Iter {i+1}/{num_iters} - Loss: {loss.item():.4f}") # Plot the convergence curves. plt.figure() for name, losses in results.items(): plt.plot(losses, label=name) plt.xlabel("Iteration") plt.ylabel("Loss") plt.title("Quadratic Function Optimization") plt.legend() plt.show()
""",
    "output_text_data": """
================================================================================ === RUNNING QUICK TEST OF ACM OPTIMIZER === ================================================================================ Python version: 3.10.16 (main, Dec 12 2024, 19:07:39) [GCC 11.4.0] PyTorch version: 2.6.0+cu124 Using device: cuda GPU: Tesla T4 GPU Memory: 16.71 GB -------------------------------------------------------------------------------- QUICK TEST CONFIGURATION: Random seed: 42 -------------------------------------------------------------------------------- ================================================================================ === QUICK TEST: SYNTHETIC OPTIMIZATION BENCHMARK === ================================================================================ Running synthetic optimization benchmarks with ACM, Adam, and SGD+momentum... Functions: Quadratic and Rosenbrock === Synthetic Optimization Benchmark === Running optimization with ACM Iter 1/20 - Loss: 2.4565 Iter 4/20 - Loss: 2.4501 Iter 8/20 - Loss: 2.4305 Iter 12/20 - Loss: 2.4024 Iter 16/20 - Loss: 2.3688 Iter 20/20 - Loss: 2.3319 Running optimization with Adam Iter 1/20 - Loss: 0.0068 Iter 4/20 - Loss: 0.0015 Iter 8/20 - Loss: -0.0056 Iter 12/20 - Loss: -0.0126 Iter 16/20 - Loss: -0.0194 Iter 20/20 - Loss: -0.0262 Running optimization with SGD_momentum Iter 1/20 - Loss: 1.0733 Iter 4/20 - Loss: 1.0335 Iter 8/20 - Loss: 0.9149 Iter 12/20 - Loss: 0.7572 Iter 16/20 - Loss: 0.5884 Iter 20/20 - Loss: 0.4260 === Rosenbrock Function Optimization === Running optimization with ACM Iter 1/20 - Loss: 1.0000 Iter 4/20 - Loss: 0.9978 Iter 8/20 - Loss: 0.9908 Iter 12/20 - Loss: 0.9808 Iter 16/20 - Loss: 0.9690 Iter 20/20 - Loss: 0.9559 Running optimization with Adam Iter 1/20 - Loss: 1.0000 Iter 4/20 - Loss: 0.9940 Iter 8/20 - Loss: 0.9861 Iter 12/20 - Loss: 0.9781 Iter 16/20 - Loss: 0.9703 Iter 20/20 - Loss: 0.9624 Running optimization with SGD_momentum Iter 1/20 - Loss: 1.0000 Iter 4/20 - Loss: 0.9777 Iter 8/20 - Loss: 0.9110 Iter 12/20 - Loss: 0.8221 Iter 16/20 - Loss: 0.7266 Iter 20/20 - Loss: 0.6347 === Synthetic Optimization Results === Quadratic Function Optimization: Final Loss Values: ACM: 2.331910 Adam: -0.026165 SGD_momentum: 0.425952 Final Positions: ACM: [0.19387202 2.1209867 ] Adam: [ 0.15900403 -0.08826145] SGD_momentum: [-0.26321486  1.0717224 ] Rosenbrock Function Optimization: Final Loss Values: ACM: 0.955919 Adam: 0.962407 SGD_momentum: 0.634735 Final Positions: ACM: [0.02402827 0.00014844] Adam: [0.01997579 0.00064254] SGD_momentum: [0.21810871 0.04335106] Convergence Analysis: Quadratic Function: ACM avg loss reduction per iteration: 0.00697672 ACM did not reach 1% of initial loss Adam avg loss reduction per iteration: 0.00126349 Adam iterations to reach 1% of initial loss: 4 SGD_momentum avg loss reduction per iteration: 0.03018586 SGD_momentum did not reach 1% of initial loss Rosenbrock Function: ACM avg loss reduction per iteration: 0.00246692 ACM did not reach 1% of initial loss Adam avg loss reduction per iteration: 0.00146969 Adam did not reach 1% of initial loss SGD_momentum avg loss reduction per iteration: 0.01706579 SGD_momentum did not reach 1% of initial loss ================================================================================ === QUICK TEST: CIFAR-10 CNN TRAINING === ================================================================================ Loading CIFAR-10 dataset... Training CNN models on CIFAR-10 with different optimizers... === CIFAR-10 CNN Training === Training with ACM optimizer Epoch 1/1 - Train Loss: 2.3023, Test Accuracy: 9.93% Training with Adam optimizer Epoch 1/1 - Train Loss: 1.5870, Test Accuracy: 54.79% Training with SGD_momentum optimizer Epoch 1/1 - Train Loss: 2.2435, Test Accuracy: 24.83% === CIFAR-10 Training Results === Final Training Loss: ACM: 2.3023 Adam: 1.5870 SGD_momentum: 2.2435 Final Test Accuracy: ACM: 9.93% Adam: 54.79% SGD_momentum: 24.83% Convergence Speed (epochs to reach 90% of final accuracy): ACM: 1 Adam: 1 SGD_momentum: 1 ================================================================================ === QUICK TEST: MNIST ABLATION STUDY === ================================================================================ Loading MNIST dataset... Running ablation study with different curvature influence parameters... === MNIST Ablation Study === Training with ACM_curv_0.0 Epoch 1/1 - Train Loss: 2.2709, Test Accuracy: 38.82% Training with ACM_curv_0.1 Epoch 1/1 - Train Loss: 2.2739, Test Accuracy: 29.41% Training with ACM_curv_1.0 Epoch 1/1 - Train Loss: 2.2359, Test Accuracy: 35.35% Training with Adam Epoch 1/1 - Train Loss: 0.2508, Test Accuracy: 98.10% Training with SGD_momentum Epoch 1/1 - Train Loss: 1.3806, Test Accuracy: 88.93% === MNIST Ablation Study Results === Final Training Loss: ACM_curv_0.0: 2.2709 ACM_curv_0.1: 2.2739 ACM_curv_1.0: 2.2359 Adam: 0.2508 SGD_momentum: 1.3806 Final Test Accuracy: ACM_curv_0.0: 38.82% ACM_curv_0.1: 29.41% ACM_curv_1.0: 35.35% Adam: 98.10% SGD_momentum: 88.93% Effect of Curvature Influence Parameter: Curvature influence = 0.0: 38.82% Curvature influence = 0.1: 29.41% Curvature influence = 1.0: 35.35% ================================================================================ === QUICK TEST COMPLETED SUCCESSFULLY === ================================================================================
""",
}
