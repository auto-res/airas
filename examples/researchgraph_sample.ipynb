{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDIoti6BoYdS"
      },
      "source": [
        "# Researchchain\n",
        "- https://pypi.org/project/researchchain/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0cDIaPGpv1n"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/auto-res/researchchain/blob/develop-tanaka/examples/researchchain_sample.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_2wTCzEa60h",
        "outputId": "756136cd-b9fa-43d6-99d4-9e4dc7d5e5c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.9/866.9 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.2/396.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade -q researchgraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMjdKgmNeIcL"
      },
      "source": [
        "# LLMComponent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nmAOawsybKeb"
      },
      "outputs": [],
      "source": [
        "from researchchain.llm_component.llm_component import LLMComponent\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get('ANTHROPIC_API_KEY')\n",
        "\n",
        "llm_name = \"gpt-4o-2024-08-06\"\n",
        "#llm_name = \"gpt-4o-2024-05-13\"\n",
        "#llm_name =  \"gpt-4o-mini-2024-07-18\"\n",
        "#llm_name = \"gpt-4-turbo-2024-04-09\"\n",
        "#llm_name = \"gpt-4-0125-preview\"\n",
        "#llm_name = \"gemini-1.0-pro\"\n",
        "#llm_name = \"gemini-1.5-pro\"\n",
        "#llm_name = \"gemini-1.5-flash\"\n",
        "#llm_name = \"claude-3-5-sonnet-20240620\"\n",
        "#llm_name = \"claude-3-opus-20240229\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbJe2QOqnvvy"
      },
      "source": [
        "### 1段階のプロンプトを実行する場合"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCdp6NoecpKX",
        "outputId": "9950d3f5-a1e6-4bdc-c95d-faa8f8ab3441"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLMComponent initialized\n",
            "input: ['source', 'language']\n",
            "output: ['output']\n"
          ]
        }
      ],
      "source": [
        "# inputとoutputを定義\n",
        "# inputからoutputを生成するためのpromptを設定\n",
        "# promptのルール\n",
        "# inputの変数と同じ文字列を{}で囲みpromptに記述(変数が埋め込まれる)\n",
        "# {}はその文字列と同じ<タグ></タグ>で囲む({source} -> <source>{source}</source>)\n",
        "# outputは出力に使う変数のタグで囲むように指示する((出力がAAAの場合) -> <AAA></AAA>で出力を囲むようにプロンプト内で指示する)\n",
        "\n",
        "json_data = {\n",
        "    \"input\": [\"source\", \"language\"],\n",
        "    \"output\": [\"translated_source\"],\n",
        "    \"prompt\": \"<source>{source}</source>\\n<language>{language}</language>\\n\\n\\nsourceタグで与えられた文章をlanguageで指定された言語に翻訳してtranslated_sourceタグを用いて出力せよ.\"\n",
        "    }\n",
        "\n",
        "translater = LLMComponent(json_data = json_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLlLZOvwkg8l",
        "outputId": "41780a6a-9b6f-41bf-972b-a9f8771363c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'source': 'Hello World!!', 'language': 'japanese', 'output': 'こんにちは、世界!!'}\n"
          ]
        }
      ],
      "source": [
        "memory = {\n",
        "    'source': 'Hello World!!',\n",
        "    'language': 'japanese',\n",
        "    }\n",
        "\n",
        "memory = translater(llm_name, memory)\n",
        "print(memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8h-SWQsnjFo"
      },
      "source": [
        "### 2段階のプロンプトを実行する場合"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBpVHqejkZ4m",
        "outputId": "0fa4fa37-4287-4dbf-f11d-30545e7c9890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLMComponent initialized\n",
            "input: [['source', 'language'], ['output2']]\n",
            "output: [['output', 'output2'], ['output3']]\n"
          ]
        }
      ],
      "source": [
        "json_data = {\n",
        "    \"input\": [\n",
        "        [\"source\", \"language\"],\n",
        "        [\"translated_source2\"]\n",
        "    ],\n",
        "    \"output\": [\n",
        "        [\"translated_source1\", \"translated_source2\"],\n",
        "        [\"translated_source3\"]\n",
        "    ],\n",
        "    \"prompt\": [\n",
        "        \"<source>{source}</source>\\n<language>{language}</language>\\n\\n\\nsource_text タグで与えられた文章をlanguageで指定された言語に翻訳してtranslated_source1タグを用いて出力せよ．さらに，翻訳したものをドイツ語に翻訳しtranslated_source2タグを用いて出力してください．\",\n",
        "        \"<translated_source2>{translated_source2}\\<translated_source2>\\n\\n\\ntranslated_source2タグで与えられた文章をフランス語に翻訳してtranslated_source3タグを用いて出力せよ．\"\n",
        "    ]\n",
        "    }\n",
        "\n",
        "translater = LLMComponent(json_data = json_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lp-xUC55k9wK",
        "outputId": "a7f8d2b2-358d-4feb-bdf1-470cb63226cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'source': 'Hello World!!', 'language': 'japanese', 'output': 'こんにちは、世界!!', 'output2': 'Hallo Welt!!', 'output3': 'Bonjour le monde !!'}\n"
          ]
        }
      ],
      "source": [
        "memory = {\n",
        "    'source': 'Hello World!!',\n",
        "    'language': 'japanese',\n",
        "    'output': None\n",
        "    }\n",
        "\n",
        "memory = translater(llm_name, memory)\n",
        "print(memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-varhAseK2F"
      },
      "source": [
        "# Retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HFakiK6hwA4"
      },
      "source": [
        "### SemanticScholarRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HCTVrOtWfBlg"
      },
      "outputs": [],
      "source": [
        "from researchchain.retriever_component.semantic_scholar import SemanticScholarRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8J7RJwHIgfrk",
        "outputId": "07b737b1-ee2d-41aa-d494-67f7994d1fc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SemanticScholarRetriever initialized\n",
            "input: ['keywords']\n",
            "output: ['collection_of_papers_1']\n"
          ]
        }
      ],
      "source": [
        "save_dir = \"/content/papers/\"\n",
        "search_variable = \"keywords\"\n",
        "output_variable = \"collection_of_papers\"\n",
        "# 検索するkeywordの数\n",
        "num_keywords = 1\n",
        "# 検索で取得する論文の数\n",
        "num_retrieve_paper = 5\n",
        "retriever = SemanticScholarRetriever(\n",
        "    save_dir=save_dir, \n",
        "    search_variable=search_variable, \n",
        "    output_variable=output_variable,\n",
        "    num_keywords=num_keywords,\n",
        "    num_retrieve_paper=num_retrieve_paper\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dd_Ykm9wgppU"
      },
      "outputs": [],
      "source": [
        "memory = {\n",
        "    'keywords' : [\"Grokking\"]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "memory = retriever(memory)\n",
        "print(memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaciTMp6hzEZ"
      },
      "source": [
        "### GithubRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QK6FA96heOYz"
      },
      "outputs": [],
      "source": [
        "from researchchain.retriever_component.github import GithubRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OuUi77Pga-O",
        "outputId": "41dadcc8-53ec-481c-922e-4f0c9445989d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GithubRetriever initialized\n",
            "input: ['github_url']\n",
            "output: ['folder_structure', 'github_file']\n"
          ]
        }
      ],
      "source": [
        "save_dir = \"/content/repository1/\"\n",
        "search_variable = 'github_url'\n",
        "output_variable = ['folder_structure', 'github_file']\n",
        "githubretriever = GithubRetriever(save_dir=save_dir, search_variable=search_variable, output_variable=output_variable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "l_kVkbWgg9J8"
      },
      "outputs": [],
      "source": [
        "memory = {\n",
        "    'github_url' : \"https://github.com/fuyu-quant/IBLM\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "oMZlOqFKiBN8"
      },
      "outputs": [],
      "source": [
        "memory = githubretriever(memory)\n",
        "print(memory)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPpO3N07SLcwA8fYIXnCagi",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
